---
title: Modern, open and downward-scaleable data engineering
subtitle: Getting started with the composable data stack
format: clean-revealjs

html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Dr Daniel Kapitan
    orcid: 0000-0001-8979-9194
    email: daniel@kapitan.net
    affiliations: Eindhoven AI Systems Institute
date: last-modified

---

## Attribution & copyright notice

::: {style="font-size: 80%;"}

This lecture is based on the following open access materials:

- Voltron Data, [The Composable Codex](https://voltrondata.com/codex/a-new-frontier)
- Cody Peterson, [Modern, hybrid, open analytics](https://anthology-of-data.science/posts/ibis-analytics/)
- Thierry Jean, [Portable dataflows with Ibis and Hamilton](https://anthology-of-data.science/posts/hamilton-ibis/)
- Documentation of the following Python libraries: [DuckDB](https://duckdb.org/docs/), [Ibis](https://ibis-project.org), [hamilton](https://github.com/dagworks-inc/hamilton), [polars](https://docs.pola.rs/), [Shiny for Python](https://shiny.posit.co/py/)

Source code: [https://github.com/anthology-of-data-science/lecture-composable-data-stack](https://github.com/anthology-of-data-science/lecture-composable-data-stack)

```{=html}
<p xmlns:cc="http://creativecommons.org/ns#" >Daniel Kapitan, <em>Modern, open and downward-scaleable data engineering</em>.<br>This work is licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>
```
:::

## Learning objectives

<br>

:::: {.columns style="font-size: 80%;"}

::: {.column width="50%"}

### Understand the problem

- how data platforms evolved in the past couple of decades
- current problems with data platforms
- what concepts underlie the composable data stack
- what concepts underlie modern data engineering workflows

:::

::: {.column width="50%"}

### Know how to

- Build end-to-end data pipeline using open source implementations of the composable data stack
- Apply best practices of functional data engineering
- Apply the split-apply-combine strategy with various syntaxes

:::
::::

::: {style="font-size: 80%;"}
### Reflect

- on how this impact your (future) work as a data engineer
- on the pro's and con's of open standards and open source

::: 

# Understand the problem
## The problem
### The ML/AI/Data (MAD) Landscape

::: {style="font-size: 60%;"}
![Source: [https://mad.firstmark.com/](https://mad.firstmark.com/)](images/open-standards-mad-landscape.jpg)
:::

## The Composable Data Management System Manifesto

<br>

::: {style="font-size: 80%"}
> _The requirement for specialization in data management systems has evolved faster than our software development practices. After decades of organic growth, this situation has created a **siloed landscape** composed of hundreds of products developed and maintained as monoliths, with limited reuse between systems. This fragmentation has resulted in developers often reinventing the wheel, **increased maintenance costs, and slowed down innovation**. It has also affected the end users, who are often required to learn the **idiosyncrasies of dozens of incompatible SQL and non-SQL API dialects**, and settle for systems with incomplete functionality and inconsistent semantics._
:::

::: {style="font-size: 60%;"}
Pedreira, Pedro, et al. [The composable data management system manifesto.](https://dl.acm.org/doi/10.14778/3603581.3603604) Proceedings of the VLDB Endowment 16.10 (2023): 2679-2685.
:::

## Let's start at the beginning
### Edgar Codd invents relational algebra (1970)

![](images/coddpaper.webp)

::: {style="font-size: 60%;"}
E. F. Codd. [A relational model of data for large shared data banks.](https://doi.org/10.1145/362384.362685) Commun. ACM 13, 6 (June 1970), 377–387. 
:::

## Standard Query Language (SQL)
### Still the most important language for a data engineer

![](images/sql-cheat-sheet.webp)

::: {style="font-size: 60%;"}
Source: Jadwiga Wilkens on Medium. [The Best BigQuery SQL Cheat Sheet for Beginners.](https://medium.com/data-school/the-best-bigquery-sql-cheat-sheet-for-beginners-81c762f72845)
:::


## Evolution of data platfom architectures

![](images/generations-data-plattforms.png)

::: {style="font-size: 60%;"}
Armbrust, Michael, et al. [Lakehouse: a new generation of open platforms that unify data warehousing and advanced analytics.](https://15721.courses.cs.cmu.edu/spring2023/papers/02-modern/armbrust-cidr21.pdf) Proceedings of CIDR. Vol. 8. 2021.
:::

## Towards a data-centric future

<br>

::: {style="font-size: 80%;"}
| NOW: Application-Centric | FUTURE: Data-Centric |
|---|---|
|Exorbitant, often  prohibitive, cost of change.	Reasonable cost of change. |Data is tied up in applications because applications own data.	Data is an open resource that outlives any given application.|
|Every new project comes with a big data conversion project.|Every new project taps into existing data stores. |
|Data exists in wide variety of heterogeneous formats, structures, meaning, and terminology.|Data is globally integrated sharing a common meaning, being exported from a common source into any needed format.|
|Data integration consumes 35%-65% of IT budget.|Data integration will be nearly free. |
|Hard or impossible to integrate external data with internal data.|Internal and external data readily integrated.|
:::

::: {style="font-size: 60%;"}
Source: [The Data-Centric Manifesto: Principles](http://datacentricmanifesto.org/principles/).
:::

## {background-color="black"}
![](images/importance-of-interoperability.png)

# Understand open standards for data platforms

## Decomposing the venerable relational database management system (RDBMS)

![](images/db-catalog-schema.png)

::: {style="font-size: 60%;"}
Source: Basil Borque on [Stackoverflow](https://stackoverflow.com/questions/7022755/whats-the-difference-between-a-catalog-and-a-schema-in-a-relational-database)
:::

## Decomposing the database into open standards
![](images/parquet-arrow-iceberg.png)

## Arrow: from row-based to column-based tables
### Columnar is faster for analytical processing

![](images/row-vs-columnar-data.png)

::: {style="font-size: 60%;"}
Source: [Apache Arrow overview](https://arrow.apache.org/overview/).
:::

## Standardization saves time and resources
### No costly serialization/deserialization, no custom implementations

<br>

:::: {.columns}
::: {.column width="50%"}
![](images/arrow-copy.png)
:::

::: {.column width="50%"}
![](images/arrow-shared.png)
:::
::::

::: {style="font-size: 60%;"}
Source: [Apache Arrow overview](https://arrow.apache.org/overview/).
:::


## JDBC/ODBC: row-based database connectivity protocols
### Less suitable for analytical workloads

![](images/adbc-flow-1.png)

::: {style="font-size: 60%;"}
Source: [Apache Arrow: Introducing ADBC](https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc/).
:::

## Arrow Database Connectivity (ADBC)
### A single API for getting Arrow data in and out of different databases

![](images/adbc-flow-2.png)

::: {style="font-size: 60%;"}
Source: [Apache Arrow: Introducing ADBC](https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc/).
:::

## Iceberg: an open table format and catalog
### Basically, a database engine in files

<br>

![](images/iceberg-metadata.png)

::: {style="font-size: 60%;"}
Source: [Apache Iceberg specification](https://iceberg.apache.org/spec/).
:::

## Iceberg catalog
### We still need a (small) database

![](images/iceberg-catalog.webp)

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::

## Iceberg metadata file
### Stores schema, partition information and snapshots

![](images/iceberg-metadata-file.webp)

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::

## Iceberg manifest list
### Stores information about each manifest file that makes up a snapshot

![](images/iceberg-manifest-list.webp)

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::


## Iceberg manifest file
### Track data files as well as additional details and statistics about each file

![](images/iceberg-manifest-file.webp)

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::

## Iceberg in practice
### Creating a table

<br>

```sql
CREATE TABLE table1 (
    order_id BIGINT,
    customer_id BIGINT,
    order_amount DECIMAL(10, 2),
    order_ts TIMESTAMP
)
USING iceberg
PARTITIONED BY ( HOUR(order_ts) );
```

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::


## Iceberg in practice
### Result of creating a table

<br>

![](images/iceberg-create-table.png)

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::

## Iceberg in practice
### Inserting data

<br>

```sql
INSERT INTO table1 VALUES (
    123,
    456,
    36.17,
    '2021-01-26 08:10:23'
);
```

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::


## Iceberg in practice
### Result of inserting data

<br>

![](images/iceberg-insert.webp)

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::

## Iceberg in practice
### Merge into/upserting data

<br>

```sql
MERGE INTO table1
USING ( SELECT * FROM table1_stage ) s
    ON table1.order_id = s.order_id
WHEN MATCHED THEN
    UPDATE table1.order_amount = s.order_amount
WHEN NOT MATCHED THEN
    INSERT *
```

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::


## Iceberg in practice
### Result of merging into/upserting data

<br>

![](images/iceberg-merge-into.png)

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::

## Iceberg in practice
### Merge into/upserting data

<br>

```sql
SELECT *
FROM db1.table1
```

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::


## Iceberg in practice
### Result of select statement

<br>

![](images/iceberg-select.png)

::: {style="font-size: 60%;"}
Source: [Apache Iceberg: An Architectural Look Under the Covers](https://www.dremio.com/resources/guides/apache-iceberg-an-architectural-look-under-the-covers/?utm_source=pocket_shared).
:::

# Understand the composable data stack

## The Data Science Hierachy of Needs
### All you need is MICE
![](/images/mice-complete-implementation.png)

## The Composable Data Stack
### Layers
![](images/composable-data-stack-layers.png)

## The Composable Data Stack
### Standards
![](images/composable-data-stack-standards.png)

## The Composable Data Stack
### Subtstrait for Intermediate Representation (IR)
![](images/composable-data-stack-implementation.png)

## The Composable Data Stack
### Arrow for connectivity and data memory layout
![](images/composable-data-stack-diverse-system-ibis-codes-written.png)


## Big Data Is Dead

![](images/big-data-is-dead.webp)

::: {style="font-size: 60%;"}
Source: Jordan Tigani, [Big Data Is Dead.](https://motherduck.com/blog/big-data-is-dead/)
:::

## You can work with 100s GB of data on a single machine
### All you need is a PC and some open source libraries
![](images/embedded-db-duckdb.png)

::: {style="font-size: 60%;"}
Source: The Data Quarry::blog [Embedded databases (1): The harmony of DuckDB, KùzuDB and LanceDB.](https://thedataquarry.com/posts/embedded-db-1/)
:::


# Know How To

## From components to a whole platform architecture

![](images/Unified-Data-Infrastructure-2.0.png)

::: {style="font-size: 60%;"}
Source: Andreessen Horowitz, [Emerging Architectures for Modern Data Infrastructure](https://a16z.com/emerging-architectures-for-modern-data-infrastructure/).
:::

## The Composable Data Stack in Practice
### Let's get into building an end-to-end stack
![](images/ibis-analytics-overview.png)

## The Pipeline Pattern
### Directed Acyclyc Graphs, ETL/ELT and functional data engineering

<br>

![](images/hamilton-architecture-overview.webp)

::: {style="font-size: 60%;"}
Source: [Hamilton](https://hamilton.dagworks.io/en/latest/).
:::

## Dagster
### Software-Defined Assets

![](images/software-defined-asset.png)

::: {style="font-size: 60%;"}
Source: Dagster [Introducing Software-Defined Assets.](https://dagster.io/blog/software-defined-assets)
:::

## Walk-through end-to-end pipeline
### Points of interest
<br>

- Overall structure of the project
- Managing your workflow: [`just`](https://just.systems/)
- Writing pipelines: functions, functions, functions
- Inspecting your DAG with Dagster
- Materializing assets
- Dashboard

## The split-apply-combine strategy for data analysis
<br>![](images/split-apply-combine.webp)

::: {style="font-size: 60%;"}
Wickham, H. (2011). The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software, 40(1), 1–29. [https://doi.org/10.18637/jss.v040.i01](https://www.jstatsoft.org/article/view/v040i01)
:::

## The split-apply-combine strategy for data analysis

### Overview data transformations in different libraries

<br>

::: {style="font-size: 80%;"}

| concept | pandas | polars | ibis | PySpark | dplyr | SQL |
| --- | --- | ---| --- | --- | --- | --- |
| **split** | groupby() | group_by() | group_by() | groupBy() | group_by() | GROUP BY |
| **combine** | join (), merge() | join() | left_join, inner_join() etc. |  join() | left_join, inner_join() etc. | LEFT JOIN, JOIN etc. |
| **filtering (row based)**| loc[], query() | filter() | filter() | filter() | filter() | WHERE | 
| **select (column based)**| loc[], iloc[],| select() | select() | select() | select() | SELECT | 
| **mutate** | assign() | with_columns() | mutuate() | withColumn() | mutate() | ADD | 
| **ordering** | sort_values() | sort() | order_by() | orderBy() | arrange() | ORDER BY |

:::
## Method chaining
### Using functions the bad way

```python
tumble_after(
    broke(
        fell_down(
            fetch(went_up(jack_jill, "hill"), "water"),
            jack),
        "crown"),
    "jill"
)
```

### ... vs. the more readable way

```python
(jack_jill
  .went_up("hill")
  .fetch("water")
  .fell_down("jack")
  .broke("crown")
  .tumble_after("jill")
```

::: {style="font-size: 60%;"}
Source: Tom's (Augspurger) Blog. [Method Chaining](https://tomaugspurger.net/posts/method-chaining/).
:::


## Naming of table hierarchy differs across backends
### Ibis uses catalog --> database --> table

::: {style="font-size: 80%;"}

| Backend    | Catalog        | Database   |
|------------|----------------|------------|
| bigquery   | project        | database   |
| clickhouse |                | database   |
| datafusion | catalog        | schema     |
| druid      | dataSourceType | dataSource |
| duckdb     | database       | schema     |
| flink      | catalog        | database   |
| impala     |                | database   |
| mssql      | database       | schema     |
| mysql      |                | database   |
| postgres   | database       | schema     |
| pyspark    |                | database   |
| snowflake  |                | database   |
| trino      | catalog        | schema     |
:::

::: {style="font-size: 60%;"}
Source: [Ibis documentation.](https://ibis-project.org/backend_table_hiearchy.html)
:::


## Working with nested data
### Gotcha! Unforunately there is no standard naming yet ...

<br>

| operation | ibis | polars | duckdb |
| --- | --- | --- | --- |
| Flatten `Array` into multiple rows | [`ArrayValue.unnest()`](https://ibis-project.org/reference/expression-collections.html#ibis.expr.types.arrays.ArrayValue.unnest) | [`DataFrame.explode()`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.explode.html#polars.DataFrame.explode) | [`UNNEST`](https://duckdb.org/docs/sql/query_syntax/unnest.html) |
| Unnest `Struct` into multiple columns | [`Table.unpack(*columns)`](https://ibis-project.org/reference/expression-tables#ibis.expr.types.relations.Table.unpack) | [`DataFrame.unnest()`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.unnest.html) | [`UNNEST`](https://duckdb.org/docs/sql/query_syntax/unnest.html) |

::: {style="font-size: 80%;"}

Ibis also has methods that operate directly on a column of structs:

- [`StructValue.destructure()`](https://ibis-project.org/reference/expression-collections#ibis.expr.types.structs.StructValue.destructure)
- [`StructValue.lift()`](https://ibis-project.org/reference/expression-collections#ibis.expr.types.structs.StructValue.lift)

:::


## You can swap components to your hearts content
### My personal preferences

<br>

| component | Ibis analytics demo | My preference |
|-----|-----|-----|
| Workflow orchestration | [Dagster](https://docs.dagster.io/getting-started) | [Hamilton](https://hamilton.dagworks.io/en/latest/) |
| Persistent storage | parquet, native DuckDB files | [Apache Iceberg](https://py.iceberg.apache.org/) |
| Dashboarding app | [Streamlit](https://docs.streamlit.io/) | [Shiny for Python](https://shiny.posit.co/py/docs/overview.html), [Quarto](https://quarto.org) |
| Visualization | [plotly](https://plotly.com/python/) | [vega-altair](https://altair-viz.github.io/) |

## Stuff we haven't covered yet
### ...and will definitely give you a headache in future projects

<br>

- [Change Data Capture (CDC)](https://en.wikipedia.org/wiki/Change_data_capture): determine and track data that has changed at the source, such that you only have to process the 'deltas'
- Setup and manage access control mechanisms in an operational data platform
- Provide documentation for non-technical users
- Reporting on data quality

# Reflect

## Group discussion
### What are your main take-outs?

## Thanks for your attention. {background-image="images/speakerscorner.jpg" background-size="contain" background-repeat="no-repeat"}
